CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.run --nproc_per_node=4 dlrm_s_pytorch.py --load-processed --processed-data-file /home/grads/d/daochen.zha/sharding_communication/sharding/data/splits/test_0 --arch-mlp-bot=512-512-16 --arch-mlp-top=512-512-1  --mini-batch-size=65536 --num-batches=10  --use-gpu --print-time --sharder input --allocation 2,3,0,3,2,0,1,0,0,1,2,1,1,2,3,0,3,1,2,3,2,2,3,0,3,0,2,3,2,3,0,1,1,1,1,3,3,1,0,2,0,1,0,1,2,2,2,0,1,1,3,1,0,3,3,3,1,2,1,3,2,2,0,0,3,2,2,1,3,1,0,2,0,0,3,1,3,0,0,2 --enable-profiling --fbgemm-emb --profile-out-dir profiles/test_0/dim_greedy/0
