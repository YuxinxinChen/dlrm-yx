CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.run --nproc_per_node=4 dlrm_s_pytorch.py --load-processed --processed-data-file /home/grads/d/daochen.zha/sharding/autoshard/data/splits/{}_{} --arch-mlp-bot=512-512-256 --arch-mlp-top=512-512-1  --mini-batch-size=16384 --num-batches=10  --use-gpu --print-time --sharder input --allocation {} --enable-profiling --fbgemm-emb --profile-out-dir profiles/{}_{}/{}/{}
